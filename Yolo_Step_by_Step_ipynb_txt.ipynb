{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Yolo Step-by-Step.ipynb..txt",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python [default]",
      "language": "python",
      "name": "python2"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/agatagruza/Microsoft-Open-Hack-AI/blob/master/Yolo_Step_by_Step_ipynb_txt.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-5GzY1A16l33",
        "colab_type": "text"
      },
      "source": [
        "Reference Help: \n",
        "[Gentle Guide on How YOLO Object Localization works with Keras](https://www.dlology.com/blog/gentle-guide-on-how-yolo-object-localization-works-with-keras-part-2/).\n",
        "\n",
        "**Outline of Steps**\n",
        "    + Initialization\n",
        "        + Download COCO detection data from http://cocodataset.org/#download\n",
        "            + http://images.cocodataset.org/zips/train2014.zip <= train images\n",
        "            + http://images.cocodataset.org/zips/val2014.zip <= validation images\n",
        "            + http://images.cocodataset.org/annotations/annotations_trainval2014.zip <= train and validation annotations\n",
        "        + Run this script to convert annotations in COCO format to VOC format\n",
        "            + https://gist.github.com/chicham/6ed3842d0d2014987186#file-coco2pascal-py\n",
        "        + Download pre-trained weights from https://pjreddie.com/darknet/yolo/\n",
        "            + https://pjreddie.com/media/files/yolo.weights\n",
        "        + Specify the directory of train annotations (train_annot_folder) and train images (train_image_folder)\n",
        "        + Specify the directory of validation annotations (valid_annot_folder) and validation images (valid_image_folder)\n",
        "        + Specity the path of pre-trained weights by setting variable *wt_path*\n",
        "    + Construct equivalent network in Keras\n",
        "        + Network arch from https://github.com/pjreddie/darknet/blob/master/cfg/yolo-voc.cfg\n",
        "    + Load the pretrained weights\n",
        "    + Perform training \n",
        "    + Perform detection on an image with newly trained weights\n",
        "    + Perform detection on an video with newly trained weights"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EkBn3KSbAM4B",
        "colab_type": "text"
      },
      "source": [
        "### To reference local files, upload with this code snippet from:\n",
        "### https://stackoverflow.com/questions/48905127/importing-py-files-in-google-colab \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7njywkdZFljW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QhNo7Svz6l35",
        "colab_type": "text"
      },
      "source": [
        "# Initialization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "frygND9B7cpf",
        "colab_type": "code",
        "outputId": "fc5815b1-4391-42a6-adbe-b3590bd0d6cd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 101
        }
      },
      "source": [
        "!git clone https://github.com/experiencor/keras-yolo2\n",
        "  "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'keras-yolo2'...\n",
            "remote: Enumerating objects: 324, done.\u001b[K\n",
            "remote: Total 324 (delta 0), reused 0 (delta 0), pack-reused 324\u001b[K\n",
            "Receiving objects: 100% (324/324), 53.89 MiB | 25.03 MiB/s, done.\n",
            "Resolving deltas: 100% (180/180), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "isEHNlz-_NNL",
        "colab_type": "code",
        "outputId": "3b4b1914-f4da-430e-a9dc-f9a19e0afa5d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "import os\n",
        "os.chdir('/content/keras-yolo2')\n",
        "%load preprocessing.py\n",
        "!echo \"Current OS Folder:\" $PWD\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Current OS Folder: /content/keras-yolo2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KJQObJfg_PWY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Initially, we must download yolov2.weights and locate in keras-yolo2 folder cloned above.\n",
        "!if [ ! -f /content/keras-yolo2/yolov2.weights ]; then echo \"fetching keras yolov2.weights file\"; wget https://pjreddie.com/media/files/yolov2.weights; fi\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "04xAT14Q_zTP",
        "colab_type": "code",
        "outputId": "4985cacc-be92-4c0d-f852-7654927b588e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "#Confirm step above has populated keras folder with expected yolov2.weights file.\n",
        "!ls /content/keras-yolo2/yolov2.weights"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/keras-yolo2/yolov2.weights\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Smync1CRvqyi",
        "colab_type": "text"
      },
      "source": [
        "# Prepare COCO Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2018-04-04T00:18:52.981155",
          "start_time": "2018-04-04T00:18:52.978076"
        },
        "id": "PhB6Uhij6l4D",
        "colab_type": "code",
        "outputId": "9073b452-3e48-4355-f75d-5061fac1b9c8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# Here we will prepare training and validation dataset, with corresponding annotations in support of later steps\n",
        "# These are located adjacent to the keras-yolo2 folder cloned above.\n",
        "\n",
        "import os\n",
        "os.chdir('/content/data')\n",
        "\n",
        "wt_path = 'yolov2.weights'\n",
        "raw_source_folder = '/content/data/val2017/'\n",
        "train_image_folder = '/content/data/train/'\n",
        "train_annot_folder = '/content/data/train_ann/'\n",
        "valid_image_folder = '/content/data/val/'\n",
        "valid_annot_folder = '/content/data/val_ann/'\n",
        "\n",
        "!mkdir -p $train_image_folder\n",
        "!mkdir -p $train_annot_folder\n",
        "!mkdir -p $valid_image_folder\n",
        "!mkdir -p $valid_annot_folder\n",
        "!ls"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "train  train_ann  val  val2017\tval2017.zip  val_ann\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TKJeGcZwkAMX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# We download COCO 2017 validation dataset into a common RAW image folder. This folder is named './val2017'\n",
        "# We then pull 10 images and place them into ./train to use during training step below.\n",
        "# Next we pull 10 images and place them into ./val to use after training is complete to validate the trained model.\n",
        "# Finally, we download the annotations and populate the ./train_ann and ./val_ann folders with relevant JSON info that aligns with the images. For now this contains more annotations than are used within our dataset.\n",
        "!rm val2017.zip\n",
        "#!rm val2017.zip.1\n",
        "!if [ ! -f /content/data/train/000000212226.jpg ]; then echo \"fetching our training and validation IMAGES (using Coco validation set).\"; wget images.cocodataset.org/zips/val2017.zip; unzip -qq val2017.zip $raw_source_folder; fi\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QAc4NaVnwQuI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#List the files we extracted\n",
        "!echo \"Contents of RAW images stored in ./val2017 folder\"\n",
        "!ls $raw_source_folder"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6NpNB2eSqby_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2qqFit6lpeZ4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#!ls /content/data/train\n",
        "#!rm /content/data/train/000000212226.jpg\n",
        "\n",
        "\n",
        "# loop over images, move first 10 from training into validation folder\n",
        "#TODO: Process the images in ./val folder to partition into TRAINING and VALIDATION sets. \n",
        "#TODO: Align the set of TRAINING/VALIDATION images to the corresponding ANNOTATIONS (populated next).\n",
        "import shutil\n",
        "import os\n",
        "\n",
        "image_folders = [train_image_folder, valid_image_folder]\n",
        "for dest_folder in image_folders:\n",
        "  files = os.listdir(raw_source_folder)\n",
        "  for f in files[0:10]:\n",
        "        shutil.move(raw_source_folder+f, dest_folder)\n",
        "\n",
        "\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KPsb5ng3wl2n",
        "colab_type": "code",
        "outputId": "f9f99de7-7dfe-4b4f-cc4f-315862ec33cb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 151
        }
      },
      "source": [
        " !echo \"images for training:\"; ls $train_image_folder\n",
        " !echo \"images for validation:\"; ls $valid_image_folder"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "images for training:\n",
            "000000136633.jpg  000000323709.jpg  000000414510.jpg  000000548780.jpg\n",
            "000000233033.jpg  000000364557.jpg  000000498286.jpg\n",
            "000000248284.jpg  000000409198.jpg  000000545958.jpg\n",
            "images for validation:\n",
            "000000074058.jpg  000000143931.jpg  000000379332.jpg  000000512330.jpg\n",
            "000000079837.jpg  000000263474.jpg  000000445248.jpg\n",
            "000000122217.jpg  000000297427.jpg  000000453341.jpg\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O1doeTWcliqE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Populate \"./annotations\" folder with Coco 2017 dataset\n",
        "\n",
        "!if [ ! -f ./annotations/captions_train2017.json ]; then echo \"fetching training and validation ANNOTATIONS\"; wget images.cocodataset.org/annotations/annotations_trainval2017.zip; unzip -n annotations_trainval2017.zip; rm annotations_trainval2017.zip; fi\n",
        "!ls ./annotations\n",
        "#Test\n",
        "#!rm annotations/captions_train2017.json"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S0LKXebd7gLB",
        "colab_type": "code",
        "outputId": "0ea2d6b6-2c79-4b5e-ca8b-60d479bcdffe",
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "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",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": ""
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 58
        }
      },
      "source": [
        "#Keeping this block just as a way to prompt to choose a file and upload it as-is to server\n",
        "from google.colab import files\n",
        "uploaded = files.upload()\n",
        "!ls yolov2.weights"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-ead3fbc7-7992-434d-8b32-a81d666a8be3\" name=\"files[]\" multiple disabled />\n",
              "     <output id=\"result-ead3fbc7-7992-434d-8b32-a81d666a8be3\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "yolov2.weights\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2Ts4RVNfk36-",
        "colab_type": "code",
        "outputId": "ddb6617b-cf26-46df-f493-6fa6fb93cda0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 101
        }
      },
      "source": [
        "!ls"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " annotations    frontend.py\t preprocessing.py   val2017\n",
            " backend.py     gen_anchors.py\t README.md\t    val2017.zip\n",
            " config.json    images\t\t requirements.txt  'Yolo Step-by-Step.ipynb'\n",
            " examples       LICENSE\t\t train.py\t    yolov2.weights\n",
            " experimental   predict.py\t utils.py\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NU7P9PN5vwhj",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2018-04-04T00:18:52.056478",
          "start_time": "2018-04-04T00:18:50.879887"
        },
        "code_folding": [],
        "scrolled": true,
        "id": "6ikKsRQe6l36",
        "colab_type": "code",
        "outputId": "9e01b774-893a-4f9d-c3b7-204d97270744",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from keras.models import Sequential, Model\n",
        "from keras.layers import Reshape, Activation, Conv2D, Input, MaxPooling2D, BatchNormalization, Flatten, Dense, Lambda\n",
        "from keras.layers.advanced_activations import LeakyReLU\n",
        "from keras.callbacks import EarlyStopping, ModelCheckpoint, TensorBoard\n",
        "from keras.optimizers import SGD, Adam, RMSprop\n",
        "from keras.layers.merge import concatenate\n",
        "import matplotlib.pyplot as plt\n",
        "import keras.backend as K\n",
        "import tensorflow as tf\n",
        "import imgaug as ia\n",
        "from tqdm import tqdm\n",
        "from imgaug import augmenters as iaa\n",
        "import numpy as np\n",
        "import pickle\n",
        "import os, cv2\n",
        "from preprocessing import parse_annotation, BatchGenerator\n",
        "from utils import WeightReader, decode_netout, draw_boxes\n",
        "\n",
        "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n",
        "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"\"\n",
        "\n",
        "%matplotlib inline"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2018-04-04T00:18:52.075535",
          "start_time": "2018-04-04T00:18:52.057712"
        },
        "scrolled": true,
        "id": "iCgUXI4n6l4A",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "LABELS = ['person', 'bicycle', 'car', 'motorcycle', 'airplane', 'bus', 'train', 'truck', 'boat', 'traffic light', 'fire hydrant', 'stop sign', 'parking meter', 'bench', 'bird', 'cat', 'dog', 'horse', 'sheep', 'cow', 'elephant', 'bear', 'zebra', 'giraffe', 'backpack', 'umbrella', 'handbag', 'tie', 'suitcase', 'frisbee', 'skis', 'snowboard', 'sports ball', 'kite', 'baseball bat', 'baseball glove', 'skateboard', 'surfboard', 'tennis racket', 'bottle', 'wine glass', 'cup', 'fork', 'knife', 'spoon', 'bowl', 'banana', 'apple', 'sandwich', 'orange', 'broccoli', 'carrot', 'hot dog', 'pizza', 'donut', 'cake', 'chair', 'couch', 'potted plant', 'bed', 'dining table', 'toilet', 'tv', 'laptop', 'mouse', 'remote', 'keyboard', 'cell phone', 'microwave', 'oven', 'toaster', 'sink', 'refrigerator', 'book', 'clock', 'vase', 'scissors', 'teddy bear', 'hair drier', 'toothbrush']\n",
        "\n",
        "IMAGE_H, IMAGE_W = 416, 416\n",
        "GRID_H,  GRID_W  = 13 , 13\n",
        "BOX              = 5\n",
        "CLASS            = len(LABELS)\n",
        "CLASS_WEIGHTS    = np.ones(CLASS, dtype='float32')\n",
        "OBJ_THRESHOLD    = 0.3#0.5\n",
        "NMS_THRESHOLD    = 0.3#0.45\n",
        "ANCHORS          = [0.57273, 0.677385, 1.87446, 2.06253, 3.33843, 5.47434, 7.88282, 3.52778, 9.77052, 9.16828]\n",
        "\n",
        "NO_OBJECT_SCALE  = 1.0\n",
        "OBJECT_SCALE     = 5.0\n",
        "COORD_SCALE      = 1.0\n",
        "CLASS_SCALE      = 1.0\n",
        "\n",
        "BATCH_SIZE       = 16\n",
        "WARM_UP_BATCHES  = 0\n",
        "TRUE_BOX_BUFFER  = 50"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VQbWgvk86l4G",
        "colab_type": "text"
      },
      "source": [
        "# Construct the network"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2018-04-04T00:18:53.978220",
          "start_time": "2018-04-04T00:18:53.967537"
        },
        "id": "YIE1KH7K6l4H",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# the function to implement the orgnization layer (thanks to github.com/allanzelener/YAD2K)\n",
        "def space_to_depth_x2(x):\n",
        "    return tf.space_to_depth(x, block_size=2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2018-04-04T00:18:58.022959",
          "start_time": "2018-04-04T00:18:55.740759"
        },
        "code_folding": [],
        "id": "35uZ2ytu6l4K",
        "colab_type": "code",
        "outputId": "5b4e923c-8788-450c-bf9d-bd568cd2915e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 289
        }
      },
      "source": [
        "input_image = Input(shape=(IMAGE_H, IMAGE_W, 3))\n",
        "true_boxes  = Input(shape=(1, 1, 1, TRUE_BOX_BUFFER , 4))\n",
        "\n",
        "# Layer 1\n",
        "x = Conv2D(32, (3,3), strides=(1,1), padding='same', name='conv_1', use_bias=False)(input_image)\n",
        "x = BatchNormalization(name='norm_1')(x)\n",
        "x = LeakyReLU(alpha=0.1)(x)\n",
        "x = MaxPooling2D(pool_size=(2, 2))(x)\n",
        "\n",
        "# Layer 2\n",
        "x = Conv2D(64, (3,3), strides=(1,1), padding='same', name='conv_2', use_bias=False)(x)\n",
        "x = BatchNormalization(name='norm_2')(x)\n",
        "x = LeakyReLU(alpha=0.1)(x)\n",
        "x = MaxPooling2D(pool_size=(2, 2))(x)\n",
        "\n",
        "# Layer 3\n",
        "x = Conv2D(128, (3,3), strides=(1,1), padding='same', name='conv_3', use_bias=False)(x)\n",
        "x = BatchNormalization(name='norm_3')(x)\n",
        "x = LeakyReLU(alpha=0.1)(x)\n",
        "\n",
        "# Layer 4\n",
        "x = Conv2D(64, (1,1), strides=(1,1), padding='same', name='conv_4', use_bias=False)(x)\n",
        "x = BatchNormalization(name='norm_4')(x)\n",
        "x = LeakyReLU(alpha=0.1)(x)\n",
        "\n",
        "# Layer 5\n",
        "x = Conv2D(128, (3,3), strides=(1,1), padding='same', name='conv_5', use_bias=False)(x)\n",
        "x = BatchNormalization(name='norm_5')(x)\n",
        "x = LeakyReLU(alpha=0.1)(x)\n",
        "x = MaxPooling2D(pool_size=(2, 2))(x)\n",
        "\n",
        "# Layer 6\n",
        "x = Conv2D(256, (3,3), strides=(1,1), padding='same', name='conv_6', use_bias=False)(x)\n",
        "x = BatchNormalization(name='norm_6')(x)\n",
        "x = LeakyReLU(alpha=0.1)(x)\n",
        "\n",
        "# Layer 7\n",
        "x = Conv2D(128, (1,1), strides=(1,1), padding='same', name='conv_7', use_bias=False)(x)\n",
        "x = BatchNormalization(name='norm_7')(x)\n",
        "x = LeakyReLU(alpha=0.1)(x)\n",
        "\n",
        "# Layer 8\n",
        "x = Conv2D(256, (3,3), strides=(1,1), padding='same', name='conv_8', use_bias=False)(x)\n",
        "x = BatchNormalization(name='norm_8')(x)\n",
        "x = LeakyReLU(alpha=0.1)(x)\n",
        "x = MaxPooling2D(pool_size=(2, 2))(x)\n",
        "\n",
        "# Layer 9\n",
        "x = Conv2D(512, (3,3), strides=(1,1), padding='same', name='conv_9', use_bias=False)(x)\n",
        "x = BatchNormalization(name='norm_9')(x)\n",
        "x = LeakyReLU(alpha=0.1)(x)\n",
        "\n",
        "# Layer 10\n",
        "x = Conv2D(256, (1,1), strides=(1,1), padding='same', name='conv_10', use_bias=False)(x)\n",
        "x = BatchNormalization(name='norm_10')(x)\n",
        "x = LeakyReLU(alpha=0.1)(x)\n",
        "\n",
        "# Layer 11\n",
        "x = Conv2D(512, (3,3), strides=(1,1), padding='same', name='conv_11', use_bias=False)(x)\n",
        "x = BatchNormalization(name='norm_11')(x)\n",
        "x = LeakyReLU(alpha=0.1)(x)\n",
        "\n",
        "# Layer 12\n",
        "x = Conv2D(256, (1,1), strides=(1,1), padding='same', name='conv_12', use_bias=False)(x)\n",
        "x = BatchNormalization(name='norm_12')(x)\n",
        "x = LeakyReLU(alpha=0.1)(x)\n",
        "\n",
        "# Layer 13\n",
        "x = Conv2D(512, (3,3), strides=(1,1), padding='same', name='conv_13', use_bias=False)(x)\n",
        "x = BatchNormalization(name='norm_13')(x)\n",
        "x = LeakyReLU(alpha=0.1)(x)\n",
        "\n",
        "skip_connection = x\n",
        "\n",
        "x = MaxPooling2D(pool_size=(2, 2))(x)\n",
        "\n",
        "# Layer 14\n",
        "x = Conv2D(1024, (3,3), strides=(1,1), padding='same', name='conv_14', use_bias=False)(x)\n",
        "x = BatchNormalization(name='norm_14')(x)\n",
        "x = LeakyReLU(alpha=0.1)(x)\n",
        "\n",
        "# Layer 15\n",
        "x = Conv2D(512, (1,1), strides=(1,1), padding='same', name='conv_15', use_bias=False)(x)\n",
        "x = BatchNormalization(name='norm_15')(x)\n",
        "x = LeakyReLU(alpha=0.1)(x)\n",
        "\n",
        "# Layer 16\n",
        "x = Conv2D(1024, (3,3), strides=(1,1), padding='same', name='conv_16', use_bias=False)(x)\n",
        "x = BatchNormalization(name='norm_16')(x)\n",
        "x = LeakyReLU(alpha=0.1)(x)\n",
        "\n",
        "# Layer 17\n",
        "x = Conv2D(512, (1,1), strides=(1,1), padding='same', name='conv_17', use_bias=False)(x)\n",
        "x = BatchNormalization(name='norm_17')(x)\n",
        "x = LeakyReLU(alpha=0.1)(x)\n",
        "\n",
        "# Layer 18\n",
        "x = Conv2D(1024, (3,3), strides=(1,1), padding='same', name='conv_18', use_bias=False)(x)\n",
        "x = BatchNormalization(name='norm_18')(x)\n",
        "x = LeakyReLU(alpha=0.1)(x)\n",
        "\n",
        "# Layer 19\n",
        "x = Conv2D(1024, (3,3), strides=(1,1), padding='same', name='conv_19', use_bias=False)(x)\n",
        "x = BatchNormalization(name='norm_19')(x)\n",
        "x = LeakyReLU(alpha=0.1)(x)\n",
        "\n",
        "# Layer 20\n",
        "x = Conv2D(1024, (3,3), strides=(1,1), padding='same', name='conv_20', use_bias=False)(x)\n",
        "x = BatchNormalization(name='norm_20')(x)\n",
        "x = LeakyReLU(alpha=0.1)(x)\n",
        "\n",
        "# Layer 21\n",
        "skip_connection = Conv2D(64, (1,1), strides=(1,1), padding='same', name='conv_21', use_bias=False)(skip_connection)\n",
        "skip_connection = BatchNormalization(name='norm_21')(skip_connection)\n",
        "skip_connection = LeakyReLU(alpha=0.1)(skip_connection)\n",
        "skip_connection = Lambda(space_to_depth_x2)(skip_connection)\n",
        "\n",
        "x = concatenate([skip_connection, x])\n",
        "\n",
        "# Layer 22\n",
        "x = Conv2D(1024, (3,3), strides=(1,1), padding='same', name='conv_22', use_bias=False)(x)\n",
        "x = BatchNormalization(name='norm_22')(x)\n",
        "x = LeakyReLU(alpha=0.1)(x)\n",
        "\n",
        "# Layer 23\n",
        "x = Conv2D(BOX * (4 + 1 + CLASS), (1,1), strides=(1,1), padding='same', name='conv_23')(x)\n",
        "output = Reshape((GRID_H, GRID_W, BOX, 4 + 1 + CLASS))(x)\n",
        "\n",
        "# small hack to allow true_boxes to be registered when Keras build the model \n",
        "# for more information: https://github.com/fchollet/keras/issues/2790\n",
        "output = Lambda(lambda args: args[0])([output, true_boxes])\n",
        "\n",
        "model = Model([input_image, true_boxes], output)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING: Logging before flag parsing goes to stderr.\n",
            "W0718 19:22:29.371722 139718027761536 deprecation_wrapper.py:119] From /usr/local/lib/python2.7/dist-packages/keras/backend/tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
            "\n",
            "W0718 19:22:29.417221 139718027761536 deprecation_wrapper.py:119] From /usr/local/lib/python2.7/dist-packages/keras/backend/tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "W0718 19:22:29.426152 139718027761536 deprecation_wrapper.py:119] From /usr/local/lib/python2.7/dist-packages/keras/backend/tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
            "\n",
            "W0718 19:22:29.476169 139718027761536 deprecation_wrapper.py:119] From /usr/local/lib/python2.7/dist-packages/keras/backend/tensorflow_backend.py:174: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
            "\n",
            "W0718 19:22:29.477560 139718027761536 deprecation_wrapper.py:119] From /usr/local/lib/python2.7/dist-packages/keras/backend/tensorflow_backend.py:181: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
            "\n",
            "W0718 19:22:29.824086 139718027761536 deprecation_wrapper.py:119] From /usr/local/lib/python2.7/dist-packages/keras/backend/tensorflow_backend.py:1834: The name tf.nn.fused_batch_norm is deprecated. Please use tf.compat.v1.nn.fused_batch_norm instead.\n",
            "\n",
            "W0718 19:22:29.916757 139718027761536 deprecation_wrapper.py:119] From /usr/local/lib/python2.7/dist-packages/keras/backend/tensorflow_backend.py:3976: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2017-11-26T12:34:03.819802Z",
          "start_time": "2017-11-26T12:34:03.786125Z"
        },
        "scrolled": false,
        "id": "XlJs6kxW6l4N",
        "colab_type": "code",
        "outputId": "ba81c13f-6f25-42f7-82e2-c6f9823a5681",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_1 (InputLayer)            (None, 416, 416, 3)  0                                            \n",
            "__________________________________________________________________________________________________\n",
            "conv_1 (Conv2D)                 (None, 416, 416, 32) 864         input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "norm_1 (BatchNormalization)     (None, 416, 416, 32) 128         conv_1[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "leaky_re_lu_1 (LeakyReLU)       (None, 416, 416, 32) 0           norm_1[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d_1 (MaxPooling2D)  (None, 208, 208, 32) 0           leaky_re_lu_1[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv_2 (Conv2D)                 (None, 208, 208, 64) 18432       max_pooling2d_1[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "norm_2 (BatchNormalization)     (None, 208, 208, 64) 256         conv_2[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "leaky_re_lu_2 (LeakyReLU)       (None, 208, 208, 64) 0           norm_2[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d_2 (MaxPooling2D)  (None, 104, 104, 64) 0           leaky_re_lu_2[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv_3 (Conv2D)                 (None, 104, 104, 128 73728       max_pooling2d_2[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "norm_3 (BatchNormalization)     (None, 104, 104, 128 512         conv_3[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "leaky_re_lu_3 (LeakyReLU)       (None, 104, 104, 128 0           norm_3[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv_4 (Conv2D)                 (None, 104, 104, 64) 8192        leaky_re_lu_3[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "norm_4 (BatchNormalization)     (None, 104, 104, 64) 256         conv_4[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "leaky_re_lu_4 (LeakyReLU)       (None, 104, 104, 64) 0           norm_4[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv_5 (Conv2D)                 (None, 104, 104, 128 73728       leaky_re_lu_4[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "norm_5 (BatchNormalization)     (None, 104, 104, 128 512         conv_5[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "leaky_re_lu_5 (LeakyReLU)       (None, 104, 104, 128 0           norm_5[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d_3 (MaxPooling2D)  (None, 52, 52, 128)  0           leaky_re_lu_5[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv_6 (Conv2D)                 (None, 52, 52, 256)  294912      max_pooling2d_3[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "norm_6 (BatchNormalization)     (None, 52, 52, 256)  1024        conv_6[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "leaky_re_lu_6 (LeakyReLU)       (None, 52, 52, 256)  0           norm_6[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv_7 (Conv2D)                 (None, 52, 52, 128)  32768       leaky_re_lu_6[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "norm_7 (BatchNormalization)     (None, 52, 52, 128)  512         conv_7[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "leaky_re_lu_7 (LeakyReLU)       (None, 52, 52, 128)  0           norm_7[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv_8 (Conv2D)                 (None, 52, 52, 256)  294912      leaky_re_lu_7[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "norm_8 (BatchNormalization)     (None, 52, 52, 256)  1024        conv_8[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "leaky_re_lu_8 (LeakyReLU)       (None, 52, 52, 256)  0           norm_8[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d_4 (MaxPooling2D)  (None, 26, 26, 256)  0           leaky_re_lu_8[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv_9 (Conv2D)                 (None, 26, 26, 512)  1179648     max_pooling2d_4[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "norm_9 (BatchNormalization)     (None, 26, 26, 512)  2048        conv_9[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "leaky_re_lu_9 (LeakyReLU)       (None, 26, 26, 512)  0           norm_9[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv_10 (Conv2D)                (None, 26, 26, 256)  131072      leaky_re_lu_9[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "norm_10 (BatchNormalization)    (None, 26, 26, 256)  1024        conv_10[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "leaky_re_lu_10 (LeakyReLU)      (None, 26, 26, 256)  0           norm_10[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "conv_11 (Conv2D)                (None, 26, 26, 512)  1179648     leaky_re_lu_10[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "norm_11 (BatchNormalization)    (None, 26, 26, 512)  2048        conv_11[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "leaky_re_lu_11 (LeakyReLU)      (None, 26, 26, 512)  0           norm_11[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "conv_12 (Conv2D)                (None, 26, 26, 256)  131072      leaky_re_lu_11[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "norm_12 (BatchNormalization)    (None, 26, 26, 256)  1024        conv_12[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "leaky_re_lu_12 (LeakyReLU)      (None, 26, 26, 256)  0           norm_12[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "conv_13 (Conv2D)                (None, 26, 26, 512)  1179648     leaky_re_lu_12[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "norm_13 (BatchNormalization)    (None, 26, 26, 512)  2048        conv_13[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "leaky_re_lu_13 (LeakyReLU)      (None, 26, 26, 512)  0           norm_13[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d_5 (MaxPooling2D)  (None, 13, 13, 512)  0           leaky_re_lu_13[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv_14 (Conv2D)                (None, 13, 13, 1024) 4718592     max_pooling2d_5[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "norm_14 (BatchNormalization)    (None, 13, 13, 1024) 4096        conv_14[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "leaky_re_lu_14 (LeakyReLU)      (None, 13, 13, 1024) 0           norm_14[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "conv_15 (Conv2D)                (None, 13, 13, 512)  524288      leaky_re_lu_14[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "norm_15 (BatchNormalization)    (None, 13, 13, 512)  2048        conv_15[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "leaky_re_lu_15 (LeakyReLU)      (None, 13, 13, 512)  0           norm_15[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "conv_16 (Conv2D)                (None, 13, 13, 1024) 4718592     leaky_re_lu_15[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "norm_16 (BatchNormalization)    (None, 13, 13, 1024) 4096        conv_16[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "leaky_re_lu_16 (LeakyReLU)      (None, 13, 13, 1024) 0           norm_16[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "conv_17 (Conv2D)                (None, 13, 13, 512)  524288      leaky_re_lu_16[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "norm_17 (BatchNormalization)    (None, 13, 13, 512)  2048        conv_17[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "leaky_re_lu_17 (LeakyReLU)      (None, 13, 13, 512)  0           norm_17[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "conv_18 (Conv2D)                (None, 13, 13, 1024) 4718592     leaky_re_lu_17[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "norm_18 (BatchNormalization)    (None, 13, 13, 1024) 4096        conv_18[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "leaky_re_lu_18 (LeakyReLU)      (None, 13, 13, 1024) 0           norm_18[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "conv_19 (Conv2D)                (None, 13, 13, 1024) 9437184     leaky_re_lu_18[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "norm_19 (BatchNormalization)    (None, 13, 13, 1024) 4096        conv_19[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "conv_21 (Conv2D)                (None, 26, 26, 64)   32768       leaky_re_lu_13[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "leaky_re_lu_19 (LeakyReLU)      (None, 13, 13, 1024) 0           norm_19[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "norm_21 (BatchNormalization)    (None, 26, 26, 64)   256         conv_21[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "conv_20 (Conv2D)                (None, 13, 13, 1024) 9437184     leaky_re_lu_19[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "leaky_re_lu_21 (LeakyReLU)      (None, 26, 26, 64)   0           norm_21[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "norm_20 (BatchNormalization)    (None, 13, 13, 1024) 4096        conv_20[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_1 (Lambda)               (None, 13, 13, 256)  0           leaky_re_lu_21[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "leaky_re_lu_20 (LeakyReLU)      (None, 13, 13, 1024) 0           norm_20[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_1 (Concatenate)     (None, 13, 13, 1280) 0           lambda_1[0][0]                   \n",
            "                                                                 leaky_re_lu_20[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv_22 (Conv2D)                (None, 13, 13, 1024) 11796480    concatenate_1[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "norm_22 (BatchNormalization)    (None, 13, 13, 1024) 4096        conv_22[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "leaky_re_lu_22 (LeakyReLU)      (None, 13, 13, 1024) 0           norm_22[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "conv_23 (Conv2D)                (None, 13, 13, 425)  435625      leaky_re_lu_22[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "reshape_1 (Reshape)             (None, 13, 13, 5, 85 0           conv_23[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "input_2 (InputLayer)            (None, 1, 1, 1, 50,  0                                            \n",
            "__________________________________________________________________________________________________\n",
            "lambda_2 (Lambda)               (None, 13, 13, 5, 85 0           reshape_1[0][0]                  \n",
            "                                                                 input_2[0][0]                    \n",
            "==================================================================================================\n",
            "Total params: 50,983,561\n",
            "Trainable params: 50,962,889\n",
            "Non-trainable params: 20,672\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cdLW4_RF6l4R",
        "colab_type": "text"
      },
      "source": [
        "# Load pretrained weights"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WKdaukSA6l4S",
        "colab_type": "text"
      },
      "source": [
        "**Load the weights originally provided by YOLO**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2018-04-04T00:18:58.168386",
          "start_time": "2018-04-04T00:18:58.110194"
        },
        "id": "WO6fj9CT6l4S",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "weight_reader = WeightReader(wt_path)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2018-04-04T00:19:04.250579",
          "start_time": "2018-04-04T00:18:58.711706"
        },
        "id": "MDG9qF146l4W",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "weight_reader.reset()\n",
        "nb_conv = 23\n",
        "\n",
        "for i in range(1, nb_conv+1):\n",
        "    conv_layer = model.get_layer('conv_' + str(i))\n",
        "    \n",
        "    if i < nb_conv:\n",
        "        norm_layer = model.get_layer('norm_' + str(i))\n",
        "        \n",
        "        size = np.prod(norm_layer.get_weights()[0].shape)\n",
        "\n",
        "        beta  = weight_reader.read_bytes(size)\n",
        "        gamma = weight_reader.read_bytes(size)\n",
        "        mean  = weight_reader.read_bytes(size)\n",
        "        var   = weight_reader.read_bytes(size)\n",
        "\n",
        "        weights = norm_layer.set_weights([gamma, beta, mean, var])       \n",
        "        \n",
        "    if len(conv_layer.get_weights()) > 1:\n",
        "        bias   = weight_reader.read_bytes(np.prod(conv_layer.get_weights()[1].shape))\n",
        "        kernel = weight_reader.read_bytes(np.prod(conv_layer.get_weights()[0].shape))\n",
        "        kernel = kernel.reshape(list(reversed(conv_layer.get_weights()[0].shape)))\n",
        "        kernel = kernel.transpose([2,3,1,0])\n",
        "        conv_layer.set_weights([kernel, bias])\n",
        "    else:\n",
        "        kernel = weight_reader.read_bytes(np.prod(conv_layer.get_weights()[0].shape))\n",
        "        kernel = kernel.reshape(list(reversed(conv_layer.get_weights()[0].shape)))\n",
        "        kernel = kernel.transpose([2,3,1,0])\n",
        "        conv_layer.set_weights([kernel])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7AfW8GIc6l4Y",
        "colab_type": "text"
      },
      "source": [
        "**Randomize weights of the last layer**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2017-11-22T14:08:00.245248Z",
          "start_time": "2017-11-22T14:08:00.215495Z"
        },
        "id": "NsiGJtkM6l4Z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "layer   = model.layers[-4] # the last convolutional layer\n",
        "weights = layer.get_weights()\n",
        "\n",
        "new_kernel = np.random.normal(size=weights[0].shape)/(GRID_H*GRID_W)\n",
        "new_bias   = np.random.normal(size=weights[1].shape)/(GRID_H*GRID_W)\n",
        "\n",
        "layer.set_weights([new_kernel, new_bias])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EujIkNIS6l4b",
        "colab_type": "text"
      },
      "source": [
        "# Perform training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lN9i7Fv56l4c",
        "colab_type": "text"
      },
      "source": [
        "**Loss function**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2017-02-01T20:44:50.211553",
          "start_time": "2017-02-01T20:44:50.206006"
        },
        "id": "aRz9snCA6l4d",
        "colab_type": "text"
      },
      "source": [
        "$$\\begin{multline}\n",
        "\\lambda_\\textbf{coord}\n",
        "\\sum_{i = 0}^{S^2}\n",
        "    \\sum_{j = 0}^{B}\n",
        "     L_{ij}^{\\text{obj}}\n",
        "            \\left[\n",
        "            \\left(\n",
        "                x_i - \\hat{x}_i\n",
        "            \\right)^2 +\n",
        "            \\left(\n",
        "                y_i - \\hat{y}_i\n",
        "            \\right)^2\n",
        "            \\right]\n",
        "\\\\\n",
        "+ \\lambda_\\textbf{coord} \n",
        "\\sum_{i = 0}^{S^2}\n",
        "    \\sum_{j = 0}^{B}\n",
        "         L_{ij}^{\\text{obj}}\n",
        "         \\left[\n",
        "        \\left(\n",
        "            \\sqrt{w_i} - \\sqrt{\\hat{w}_i}\n",
        "        \\right)^2 +\n",
        "        \\left(\n",
        "            \\sqrt{h_i} - \\sqrt{\\hat{h}_i}\n",
        "        \\right)^2\n",
        "        \\right]\n",
        "\\\\\n",
        "+ \\sum_{i = 0}^{S^2}\n",
        "    \\sum_{j = 0}^{B}\n",
        "        L_{ij}^{\\text{obj}}\n",
        "        \\left(\n",
        "            C_i - \\hat{C}_i\n",
        "        \\right)^2\n",
        "\\\\\n",
        "+ \\lambda_\\textrm{noobj}\n",
        "\\sum_{i = 0}^{S^2}\n",
        "    \\sum_{j = 0}^{B}\n",
        "    L_{ij}^{\\text{noobj}}\n",
        "        \\left(\n",
        "            C_i - \\hat{C}_i\n",
        "        \\right)^2\n",
        "\\\\\n",
        "+ \\sum_{i = 0}^{S^2}\n",
        "L_i^{\\text{obj}}\n",
        "    \\sum_{c \\in \\textrm{classes}}\n",
        "        \\left(\n",
        "            p_i(c) - \\hat{p}_i(c)\n",
        "        \\right)^2\n",
        "\\end{multline}$$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2017-11-26T12:34:28.064549Z",
          "start_time": "2017-11-26T12:34:27.800510Z"
        },
        "code_folding": [],
        "id": "GE96_sKE6l4e",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def custom_loss(y_true, y_pred):\n",
        "    mask_shape = tf.shape(y_true)[:4]\n",
        "    \n",
        "    cell_x = tf.to_float(tf.reshape(tf.tile(tf.range(GRID_W), [GRID_H]), (1, GRID_H, GRID_W, 1, 1)))\n",
        "    cell_y = tf.transpose(cell_x, (0,2,1,3,4))\n",
        "\n",
        "    cell_grid = tf.tile(tf.concat([cell_x,cell_y], -1), [BATCH_SIZE, 1, 1, 5, 1])\n",
        "    \n",
        "    coord_mask = tf.zeros(mask_shape)\n",
        "    conf_mask  = tf.zeros(mask_shape)\n",
        "    class_mask = tf.zeros(mask_shape)\n",
        "    \n",
        "    seen = tf.Variable(0.)\n",
        "    total_recall = tf.Variable(0.)\n",
        "    \n",
        "    \"\"\"\n",
        "    Adjust prediction\n",
        "    \"\"\"\n",
        "    ### adjust x and y      \n",
        "    pred_box_xy = tf.sigmoid(y_pred[..., :2]) + cell_grid\n",
        "    \n",
        "    ### adjust w and h\n",
        "    pred_box_wh = tf.exp(y_pred[..., 2:4]) * np.reshape(ANCHORS, [1,1,1,BOX,2])\n",
        "    \n",
        "    ### adjust confidence\n",
        "    pred_box_conf = tf.sigmoid(y_pred[..., 4])\n",
        "    \n",
        "    ### adjust class probabilities\n",
        "    pred_box_class = y_pred[..., 5:]\n",
        "    \n",
        "    \"\"\"\n",
        "    Adjust ground truth\n",
        "    \"\"\"\n",
        "    ### adjust x and y\n",
        "    true_box_xy = y_true[..., 0:2] # relative position to the containing cell\n",
        "    \n",
        "    ### adjust w and h\n",
        "    true_box_wh = y_true[..., 2:4] # number of cells accross, horizontally and vertically\n",
        "    \n",
        "    ### adjust confidence\n",
        "    true_wh_half = true_box_wh / 2.\n",
        "    true_mins    = true_box_xy - true_wh_half\n",
        "    true_maxes   = true_box_xy + true_wh_half\n",
        "    \n",
        "    pred_wh_half = pred_box_wh / 2.\n",
        "    pred_mins    = pred_box_xy - pred_wh_half\n",
        "    pred_maxes   = pred_box_xy + pred_wh_half       \n",
        "    \n",
        "    intersect_mins  = tf.maximum(pred_mins,  true_mins)\n",
        "    intersect_maxes = tf.minimum(pred_maxes, true_maxes)\n",
        "    intersect_wh    = tf.maximum(intersect_maxes - intersect_mins, 0.)\n",
        "    intersect_areas = intersect_wh[..., 0] * intersect_wh[..., 1]\n",
        "    \n",
        "    true_areas = true_box_wh[..., 0] * true_box_wh[..., 1]\n",
        "    pred_areas = pred_box_wh[..., 0] * pred_box_wh[..., 1]\n",
        "\n",
        "    union_areas = pred_areas + true_areas - intersect_areas\n",
        "    iou_scores  = tf.truediv(intersect_areas, union_areas)\n",
        "    \n",
        "    true_box_conf = iou_scores * y_true[..., 4]\n",
        "    \n",
        "    ### adjust class probabilities\n",
        "    true_box_class = tf.argmax(y_true[..., 5:], -1)\n",
        "    \n",
        "    \"\"\"\n",
        "    Determine the masks\n",
        "    \"\"\"\n",
        "    ### coordinate mask: simply the position of the ground truth boxes (the predictors)\n",
        "    coord_mask = tf.expand_dims(y_true[..., 4], axis=-1) * COORD_SCALE\n",
        "    \n",
        "    ### confidence mask: penelize predictors + penalize boxes with low IOU\n",
        "    # penalize the confidence of the boxes, which have IOU with some ground truth box < 0.6\n",
        "    true_xy = true_boxes[..., 0:2]\n",
        "    true_wh = true_boxes[..., 2:4]\n",
        "    \n",
        "    true_wh_half = true_wh / 2.\n",
        "    true_mins    = true_xy - true_wh_half\n",
        "    true_maxes   = true_xy + true_wh_half\n",
        "    \n",
        "    pred_xy = tf.expand_dims(pred_box_xy, 4)\n",
        "    pred_wh = tf.expand_dims(pred_box_wh, 4)\n",
        "    \n",
        "    pred_wh_half = pred_wh / 2.\n",
        "    pred_mins    = pred_xy - pred_wh_half\n",
        "    pred_maxes   = pred_xy + pred_wh_half    \n",
        "    \n",
        "    intersect_mins  = tf.maximum(pred_mins,  true_mins)\n",
        "    intersect_maxes = tf.minimum(pred_maxes, true_maxes)\n",
        "    intersect_wh    = tf.maximum(intersect_maxes - intersect_mins, 0.)\n",
        "    intersect_areas = intersect_wh[..., 0] * intersect_wh[..., 1]\n",
        "    \n",
        "    true_areas = true_wh[..., 0] * true_wh[..., 1]\n",
        "    pred_areas = pred_wh[..., 0] * pred_wh[..., 1]\n",
        "\n",
        "    union_areas = pred_areas + true_areas - intersect_areas\n",
        "    iou_scores  = tf.truediv(intersect_areas, union_areas)\n",
        "\n",
        "    best_ious = tf.reduce_max(iou_scores, axis=4)\n",
        "    conf_mask = conf_mask + tf.to_float(best_ious < 0.6) * (1 - y_true[..., 4]) * NO_OBJECT_SCALE\n",
        "    \n",
        "    # penalize the confidence of the boxes, which are reponsible for corresponding ground truth box\n",
        "    conf_mask = conf_mask + y_true[..., 4] * OBJECT_SCALE\n",
        "    \n",
        "    ### class mask: simply the position of the ground truth boxes (the predictors)\n",
        "    class_mask = y_true[..., 4] * tf.gather(CLASS_WEIGHTS, true_box_class) * CLASS_SCALE       \n",
        "    \n",
        "    \"\"\"\n",
        "    Warm-up training\n",
        "    \"\"\"\n",
        "    no_boxes_mask = tf.to_float(coord_mask < COORD_SCALE/2.)\n",
        "    seen = tf.assign_add(seen, 1.)\n",
        "    \n",
        "    true_box_xy, true_box_wh, coord_mask = tf.cond(tf.less(seen, WARM_UP_BATCHES), \n",
        "                          lambda: [true_box_xy + (0.5 + cell_grid) * no_boxes_mask, \n",
        "                                   true_box_wh + tf.ones_like(true_box_wh) * np.reshape(ANCHORS, [1,1,1,BOX,2]) * no_boxes_mask, \n",
        "                                   tf.ones_like(coord_mask)],\n",
        "                          lambda: [true_box_xy, \n",
        "                                   true_box_wh,\n",
        "                                   coord_mask])\n",
        "    \n",
        "    \"\"\"\n",
        "    Finalize the loss\n",
        "    \"\"\"\n",
        "    nb_coord_box = tf.reduce_sum(tf.to_float(coord_mask > 0.0))\n",
        "    nb_conf_box  = tf.reduce_sum(tf.to_float(conf_mask  > 0.0))\n",
        "    nb_class_box = tf.reduce_sum(tf.to_float(class_mask > 0.0))\n",
        "    \n",
        "    loss_xy    = tf.reduce_sum(tf.square(true_box_xy-pred_box_xy)     * coord_mask) / (nb_coord_box + 1e-6) / 2.\n",
        "    loss_wh    = tf.reduce_sum(tf.square(true_box_wh-pred_box_wh)     * coord_mask) / (nb_coord_box + 1e-6) / 2.\n",
        "    loss_conf  = tf.reduce_sum(tf.square(true_box_conf-pred_box_conf) * conf_mask)  / (nb_conf_box  + 1e-6) / 2.\n",
        "    loss_class = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=true_box_class, logits=pred_box_class)\n",
        "    loss_class = tf.reduce_sum(loss_class * class_mask) / (nb_class_box + 1e-6)\n",
        "    \n",
        "    loss = loss_xy + loss_wh + loss_conf + loss_class\n",
        "    \n",
        "    nb_true_box = tf.reduce_sum(y_true[..., 4])\n",
        "    nb_pred_box = tf.reduce_sum(tf.to_float(true_box_conf > 0.5) * tf.to_float(pred_box_conf > 0.3))\n",
        "\n",
        "    \"\"\"\n",
        "    Debugging code\n",
        "    \"\"\"    \n",
        "    current_recall = nb_pred_box/(nb_true_box + 1e-6)\n",
        "    total_recall = tf.assign_add(total_recall, current_recall) \n",
        "\n",
        "    loss = tf.Print(loss, [tf.zeros((1))], message='Dummy Line \\t', summarize=1000)\n",
        "    loss = tf.Print(loss, [loss_xy], message='Loss XY \\t', summarize=1000)\n",
        "    loss = tf.Print(loss, [loss_wh], message='Loss WH \\t', summarize=1000)\n",
        "    loss = tf.Print(loss, [loss_conf], message='Loss Conf \\t', summarize=1000)\n",
        "    loss = tf.Print(loss, [loss_class], message='Loss Class \\t', summarize=1000)\n",
        "    loss = tf.Print(loss, [loss], message='Total Loss \\t', summarize=1000)\n",
        "    loss = tf.Print(loss, [current_recall], message='Current Recall \\t', summarize=1000)\n",
        "    loss = tf.Print(loss, [total_recall/seen], message='Average Recall \\t', summarize=1000)\n",
        "    \n",
        "    return loss"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v51nssq_6l4h",
        "colab_type": "text"
      },
      "source": [
        "**Parse the annotations to construct train generator and validation generator**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2017-11-26T12:38:44.283547Z",
          "start_time": "2017-11-26T12:38:44.277155Z"
        },
        "id": "y-zUE0vd6l4i",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "generator_config = {\n",
        "    'IMAGE_H'         : IMAGE_H, \n",
        "    'IMAGE_W'         : IMAGE_W,\n",
        "    'GRID_H'          : GRID_H,  \n",
        "    'GRID_W'          : GRID_W,\n",
        "    'BOX'             : BOX,\n",
        "    'LABELS'          : LABELS,\n",
        "    'CLASS'           : len(LABELS),\n",
        "    'ANCHORS'         : ANCHORS,\n",
        "    'BATCH_SIZE'      : BATCH_SIZE,\n",
        "    'TRUE_BOX_BUFFER' : 50,\n",
        "}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mGi5C-QX6l4l",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def normalize(image):\n",
        "    return image / 255."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bjbEFquJFzwy",
        "colab_type": "text"
      },
      "source": [
        "**TODO: This step uses a 12GB training set.. is this the approach we want to use?**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sqT6xDbwjqKu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2017-11-26T12:38:51.836129Z",
          "start_time": "2017-11-26T12:38:51.766843Z"
        },
        "id": "mzwhcRiv6l4o",
        "colab_type": "code",
        "outputId": "a293e86e-3ca7-40b1-d5a5-510c66a9e405",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 346
        }
      },
      "source": [
        "train_imgs, seen_train_labels = parse_annotation(train_annot_folder, train_image_folder, labels=LABELS)\n",
        "### write parsed annotations to pickle for fast retrieval next time\n",
        "with open('train_imgs', 'wb') as fp:\n",
        "    pickle.dump(train_imgs, fp)\n",
        "\n",
        "### read saved pickle of parsed annotations\n",
        "with open ('train_imgs', 'rb') as fp:\n",
        "    train_imgs = pickle.load(fp)\n",
        "train_batch = BatchGenerator(train_imgs, generator_config, norm=normalize)\n",
        "\n",
        "valid_imgs, seen_valid_labels = parse_annotation(valid_annot_folder, valid_image_folder, labels=LABELS)\n",
        "### write parsed annotations to pickle for fast retrieval next time\n",
        "#with open('valid_imgs', 'wb') as fp:\n",
        "#    pickle.dump(valid_imgs, fp)\n",
        "\n",
        "### read saved pickle of parsed annotations\n",
        "#with open ('valid_imgs', 'rb') as fp:\n",
        "#    valid_imgs = pickle.load(fp)\n",
        "valid_batch = BatchGenerator(valid_imgs, generator_config, norm=normalize, jitter=False)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "error",
          "ename": "OSError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m\u001b[0m",
            "\u001b[0;31mOSError\u001b[0mTraceback (most recent call last)",
            "\u001b[0;32m<ipython-input-21-645b4c9b22fc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain_imgs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseen_train_labels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparse_annotation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_annot_folder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_image_folder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mLABELS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;31m### write parsed annotations to pickle for fast retrieval next time\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'train_imgs'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'wb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfp\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_imgs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/keras-yolo2/preprocessing.py\u001b[0m in \u001b[0;36mparse_annotation\u001b[0;34m(ann_dir, img_dir, labels)\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0mseen_labels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mann\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msorted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mann_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m         \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m'object'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mOSError\u001b[0m: [Errno 2] No such file or directory: '/home/toby/data/coco/train2017ann/'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BIEvj6OO6l4q",
        "colab_type": "text"
      },
      "source": [
        "**Setup a few callbacks and start the training**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2017-11-26T12:38:15.714460Z",
          "start_time": "2017-11-26T12:38:15.708674Z"
        },
        "code_folding": [],
        "id": "6yQPOq1j6l4s",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "early_stop = EarlyStopping(monitor='val_loss', \n",
        "                           min_delta=0.001, \n",
        "                           patience=3, \n",
        "                           mode='min', \n",
        "                           verbose=1)\n",
        "\n",
        "checkpoint = ModelCheckpoint('weights_coco.h5', \n",
        "                             monitor='val_loss', \n",
        "                             verbose=1, \n",
        "                             save_best_only=True, \n",
        "                             mode='min', \n",
        "                             period=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "start_time": "2017-11-26T20:38:54.037Z"
        },
        "scrolled": false,
        "id": "4sMX8ts86l4w",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tb_counter  = len([log for log in os.listdir(os.path.expanduser('~/logs/')) if 'coco_' in log]) + 1\n",
        "tensorboard = TensorBoard(log_dir=os.path.expanduser('~/logs/') + 'coco_' + '_' + str(tb_counter), \n",
        "                          histogram_freq=0, \n",
        "                          write_graph=True, \n",
        "                          write_images=False)\n",
        "\n",
        "optimizer = Adam(lr=0.5e-4, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.0)\n",
        "#optimizer = SGD(lr=1e-4, decay=0.0005, momentum=0.9)\n",
        "#optimizer = RMSprop(lr=1e-4, rho=0.9, epsilon=1e-08, decay=0.0)\n",
        "\n",
        "model.compile(loss=custom_loss, optimizer=optimizer)\n",
        "\n",
        "model.fit_generator(generator        = train_batch, \n",
        "                    steps_per_epoch  = len(train_batch), \n",
        "                    epochs           = 100, \n",
        "                    verbose          = 1,\n",
        "                    validation_data  = valid_batch,\n",
        "                    validation_steps = len(valid_batch),\n",
        "                    callbacks        = [early_stop, checkpoint, tensorboard], \n",
        "                    max_queue_size   = 3)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y307_Olm6l41",
        "colab_type": "text"
      },
      "source": [
        "# Perform detection on image"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2017-11-22T14:07:49.271978Z",
          "start_time": "2017-11-22T14:07:49.268999Z"
        },
        "id": "I0v2glDB6l42",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.load_weights(\"weights_coco.h5\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2018-04-04T00:19:07.263359",
          "start_time": "2018-04-04T00:19:05.658285"
        },
        "scrolled": false,
        "id": "zztuCZlk6l44",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "image = cv2.imread('images/giraffe.jpg')\n",
        "dummy_array = np.zeros((1,1,1,1,TRUE_BOX_BUFFER,4))\n",
        "\n",
        "plt.figure(figsize=(10,10))\n",
        "\n",
        "input_image = cv2.resize(image, (416, 416))\n",
        "input_image = input_image / 255.\n",
        "input_image = input_image[:,:,::-1]\n",
        "input_image = np.expand_dims(input_image, 0)\n",
        "\n",
        "netout = model.predict([input_image, dummy_array])\n",
        "\n",
        "boxes = decode_netout(netout[0], \n",
        "                      obj_threshold=OBJ_THRESHOLD,\n",
        "                      nms_threshold=NMS_THRESHOLD,\n",
        "                      anchors=ANCHORS, \n",
        "                      nb_class=CLASS)\n",
        "            \n",
        "image = draw_boxes(image, boxes, labels=LABELS)\n",
        "\n",
        "plt.imshow(image[:,:,::-1]); plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RLNpgWYt6l47",
        "colab_type": "text"
      },
      "source": [
        "# Perform detection on video"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2017-10-06T13:28:28.029334Z",
          "start_time": "2017-10-06T13:28:28.024662Z"
        },
        "id": "zxyQDcEH6l48",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.load_weights(\"weights_coco.h5\")\n",
        "\n",
        "dummy_array = np.zeros((1,1,1,1,TRUE_BOX_BUFFER,4))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2017-10-06T13:39:09.640646Z",
          "start_time": "2017-10-06T13:31:44.627609Z"
        },
        "id": "wZLZW-Dx6l4_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "video_inp = '../basic-yolo-keras/images/phnom_penh.mp4'\n",
        "video_out = '../basic-yolo-keras/images/phnom_penh_bbox.mp4'\n",
        "\n",
        "video_reader = cv2.VideoCapture(video_inp)\n",
        "\n",
        "nb_frames = int(video_reader.get(cv2.CAP_PROP_FRAME_COUNT))\n",
        "frame_h = int(video_reader.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
        "frame_w = int(video_reader.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
        "\n",
        "video_writer = cv2.VideoWriter(video_out,\n",
        "                               cv2.VideoWriter_fourcc(*'XVID'), \n",
        "                               50.0, \n",
        "                               (frame_w, frame_h))\n",
        "\n",
        "for i in tqdm(range(nb_frames)):\n",
        "    ret, image = video_reader.read()\n",
        "    \n",
        "    input_image = cv2.resize(image, (416, 416))\n",
        "    input_image = input_image / 255.\n",
        "    input_image = input_image[:,:,::-1]\n",
        "    input_image = np.expand_dims(input_image, 0)\n",
        "\n",
        "    netout = model.predict([input_image, dummy_array])\n",
        "\n",
        "    boxes = decode_netout(netout[0], \n",
        "                          obj_threshold=0.3,\n",
        "                          nms_threshold=NMS_THRESHOLD,\n",
        "                          anchors=ANCHORS, \n",
        "                          nb_class=CLASS)\n",
        "    image = draw_boxes(image, boxes, labels=LABELS)\n",
        "\n",
        "    video_writer.write(np.uint8(image))\n",
        "    \n",
        "video_reader.release()\n",
        "video_writer.release()  "
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}